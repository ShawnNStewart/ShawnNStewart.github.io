[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "I am the Assistant Director for Data Analysis at the University of Texas at Dallas International Center, and I am completing my MS in Social Data Analytics and Research in spring 2023. I am passionate about using data to solve problems and generate new insights to guide decision making."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am the Assistant Director for Data Analysis at the University of Texas at Dallas International Center, and I am completing my MS in Social Data Analytics and Research in spring 2023. I am passionate about using data to solve problems and generate new insights to guide decision making."
  },
  {
    "objectID": "Assignments.html",
    "href": "Assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Assignment-2: Exploratory Analysis of TEDS-2016 data\n\n\n\nEPPS 6323\n\n\nassignment\n\n\n\nRun an exploratory data analysis with R using the TEDS2016 dataset.\n\n\n\nShawn Stewart\n\n\nFeb 15, 2023\n\n\n\n\n\n\n\n\n\n\n\nCombining tools for better data science\n\n\n\nEPPS 6323\n\n\nassignment\n\n\n\nThis review explores perspectives from Breiman and Schmueli on the differences and uses for traditional statistical modeling (ie, regression) and newer predictive modeling…\n\n\n\nShawn Stewart\n\n\nFeb 19, 2023\n\n\n\n\n\n\n\n\n\n\n\nR Programming Basic Commands - Lab 02\n\n\n\nEPPS 6323\n\n\nlab\n\n\n\nLab materials provided by Dr. Ho in class.\n\n\n\nKarl Ho\n\n\n\n\n\n\n\n\n\n\n\nEPPS 6323: Lab03\n\n\n\nEPPS 6323\n\n\nlab\n\n\n\n\n\n\n\nShawn Stewart\n\n\n\n\n\n\n\n\n\n\n\nAssignment 3 - Regression Analysis with TEDS 2016 data\n\n\n\nEPPS 6323\n\n\nlab\n\n\n\n\n\n\n\nShawn Stewart\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to R - Lab01\n\n\n\nEPPS 6323\n\n\nlab\n\n\n\nLab materials provided by Dr. Ho in class.\n\n\n\nKarl Ho\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/EPPS6323_Assignment1.html",
    "href": "posts/EPPS6323_Assignment1.html",
    "title": "Combining tools for better data science",
    "section": "",
    "text": "In his incisive 2001 article on the “two cultures” of statistical modeling, Breiman noted the almost exclusive use of traditional statistical techniques (ie, linear regression models) in research at the time and then details all the issues he sees with using linear models (and especially, using them exclusively). He asserts that the assumptions we must make when using linear models are not tenable and the methods we have for testing the validity of linear models are insufficient. Breiman sees algorithmic predictive models as the superior approach, and admonishes fellows in his field to move towards predictive modeling.\nIn Schmueli’s 2010 article on the difference between explanatory models and predictive models, it seems not a lot has changed in terms of statisticians’ preferences for modeling. Schmueli also comments on how regression models are used almost exclusively, to the detriment of the research. However, he advocates for combining the two - gettings the best of both worlds. He stresses that explanatory and predictive models have different uses, different strengths, and should be used in different circumstances. Often, they can work together to provide a more complete picture than either one could alone. For example, with the explosion in both the volume and type of data in recent years, predictive models can help uncover new trends and associations that we might not be able to uncover simply through reviewing the theory, literature, and running regression models. The insights we gain from predictive modelling can then feed into explanatory models.\nSchmueli also advocates for more inter-disciplinary collaborations. As both he and Breiman point out, predictive modeling is more often found in the domain of computer scientists, and statisticians and social science researchers often do not venture into this domain. All parties could improve their research and find value in cross-domain collaborations.\nReferences Breiman, L. (2001). Statistical modeling: The two cultures (with comments and a rejoinder by the author). Statistical science, 16(3), 199-231.\nShmueli, G. (2010). To explain or to predict?."
  },
  {
    "objectID": "Projects.html",
    "href": "Projects.html",
    "title": "Projects",
    "section": "",
    "text": "Project for EPPS 6356 - Data Visualization. For this project, I used the TransPop dataset (downloaded from ICPSR). I used the survey and srvyr packages to work with weighted data and developed visualizations using ggplot2.\n\n\n\nProject for EPPS 6316 - Regression Analysis. In this course, we were tasked with finding a study and replicating its results using R, then extending the study with our own regression analysis. I replicated Burford et al’s study Associations of Urbanicity and Sociodemographic Characteristics with Protective Health Behaviors and Reasons for Leaving the Home During COVID-19. You can download my dataset and R script."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nUniversity of Texas at Dallas | Richardson, TX BA Literary Studies | December 2010\nUniversity of Texas at Dallas | Richardson, TX MS Social Data Analytics and Research | May 2023"
  },
  {
    "objectID": "posts/Lab02.html",
    "href": "posts/Lab02.html",
    "title": "R Programming Basic Commands - Lab 02",
    "section": "",
    "text": "(Adapted from ISLR Chapter 3 Lab: Introduction to R)\n\n\n\nA=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),]\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A)\n\n[1] 4 4\n\n\n\n\n\n\nAuto=read.table(\"https://raw.githubusercontent.com/karlho/knowledgemining/gh-pages/data/Auto.data\")\n# fix(Auto) # Starting the X11 R data editor\nAuto=read.table(\"https://raw.githubusercontent.com/karlho/knowledgemining/gh-pages/data/Auto.data\",header=T,na.strings=\"?\")\n# fix(Auto)\nAuto=read.csv(\"https://raw.githubusercontent.com/karlho/knowledgemining/gh-pages/data/Auto.csv\",header=T,na.strings=\"?\")\n# fix(Auto)\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,]\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto)\n\n[1] 392   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\"        \n\n\n\n\n\n\nAuto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9\n\n\n\n\n\n\n# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\nhist(mpg)\n\n\n\nhist(mpg,col=2)\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\nplot(horsepower,mpg)\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60 \n\n\n\n\n\n\nptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\n\nThe downloaded binary packages are in\n    /var/folders/lp/fy668bhn4jvcgy6x11n3_h4m0000gn/T//Rtmp9yNNSr/downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nLoading required package: ISLR\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 34.55384    0.56263   61.41   <2e-16 ***\nlstat       -0.95005    0.03873  -24.53   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: < 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375 \n\n\n\n\n\n\n\n\n\nlm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 33.22276    0.73085  45.458  < 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  < 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: < 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  < 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: < 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  < 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: < 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)\n\n\n\n\n\nlm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 42.862007   0.872084   49.15   <2e-16 ***\nlstat       -2.332821   0.123803  -18.84   <2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: < 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(>F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       22.5328     0.2318  97.197  < 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  < 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  < 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: < 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -76.488      5.028  -15.21   <2e-16 ***\nlog(rm)       54.055      2.739   19.73   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\n# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  < 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  < 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  < 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  < 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: < 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\n\n\n\n\nsummary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  < 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "posts/Lab01.html",
    "href": "posts/Lab01.html",
    "title": "Introduction to R - Lab01",
    "section": "",
    "text": "x <- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)\n\n\n\n\n\nlength(x)  # What does length() do?\n\n[1] 3\n\nlength(y)\n\n[1] 3\n\n\n\n\n\n\nx+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"x\" \"y\"\n\nrm(x,y) # Remove objects\nls()\n\ncharacter(0)\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!\n\n\n\n\n\n?matrix\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=TRUE) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9943318\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)\n\n\n\n\n\nmean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768\n\n\n\n\n\n\nx=rnorm(100)\ny=rnorm(100)\nplot(x,y, pch=, col = \"steelblue\") # Scatterplot for two numeric variables by default\n\n\n\nplot(x,y, pch=20, col = \"red\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\nquartz_off_screen \n                2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nUniversity of Texas at Dallas | Richardson, TX BA Literary Studies | December 2010\nUniversity of Texas at Dallas | Richardson, TX MS Social Data Analytics and Research | May 2023"
  },
  {
    "objectID": "projects/InteractiveMap.html",
    "href": "projects/InteractiveMap.html",
    "title": "Interactive Map Generator",
    "section": "",
    "text": "For my fall 2022 Python Programming course (EPPS 6317), we were tasked with creating a tool using what we had learned about Python. The goal was to create a reusable script. For my project, I created a script in Jupyter Notebook that could be used to take a file input, generate an interactive map, and allow the user to name and save it as an HTML file to be included on the ISSO website. For more details, see the readme file.\n\nInteractive Map\nUpdated 12/10/2022 Author: Shawn Stewart\nEvery year after fall census day, we use the OSPA report to update our fact sheets, data for Open Doors, and other resources. We will use this same data to update our interactive map on our website. This program will take the OSPA data, process it, and generate an HTML map that can be embedded on the ISSO webpage.\nFirst, get the OSPA data in CSV format and save it to your computer.\nThen, run each cell below by typing “shift + enter” and enter information as prompted. It is important to run the cells in order.\nYou will be asked to select a file (the csv OSPA report you just saved) and enter a name for your final output, as well as a directory to save it in.\n\nImport modules\nThis cell sets up the libraries required to run the code. Press “shift + enter” to continue.\nimport geopandas\n#import fiona\nimport folium \nfrom folium.plugins import MarkerCluster\nimport pandas as pd\nimport geopandas as gpd\nimport requests\nimport json\nimport tkinter as tk\nfrom tkinter import filedialog\n# include this, otherwise an empty box will remain on screen after selecting file\nroot = tk.Tk()\nroot.withdraw()\nimport re\nimport os\n%matplotlib inline\n\n\nFile input\n# prompt user to select file, limited to only csv files\nfile = filedialog.askopenfilename(filetypes=[(\"csv files\", \"*.csv\")])\n\n\nRead in the data\n# check that the user entered a file. If not, prompt again.\nwhile not file:\n    print(\"You must choose a file to continue\")\n    file = filedialog.askopenfilename(filetypes=[(\"csv files\", \"*.csv\")])\n# read in the data\ndf = pd.read_csv(file)\n# keep only the columns we care about, and keep only records for F visas.\ndf = (df[[\"COUNTRY_DESC_PS\", \"GENDER\", \n          \"ACAD_PROG\", \"ACAD_GROUP\", \n          \"ACAD_PLAN_DESC\", \"OPT\", \"VISA_CATEGORY\"]]\n      .loc[df.VISA_CATEGORY == \"F\"])\n# get list of unique countries to pass to API\ncountry_list = set(df[\"COUNTRY_DESC_PS\"])\n\n\nGet longitude/latitude from Nominatim API\nThis can take a few minutes. If there is an asterisk next to the code block, it’s running.\nSee the Nominatim API documentation for more details on how the search query is structured - https://nominatim.org/release-docs/develop/api/Search/\n# create new data frame with long/lat for each country using Nominatim API\n# set base URL\nbase_url = \"https://nominatim.openstreetmap.org/search\"\n\n# create country dictionary\ncountry_list_dicts = []\n\n# run through each country, getting long/lat for each\nfor country in country_list:\n    # set search parameters\n    search_params = {\"country\": country,\n                     \"format\": \"json\",\n                     # Nominatim requires an email to track usage.\n                     \"email\": \"ICTechTeam@utdallas.edu\"}\n    # make request\n    r = requests.get(base_url, params=search_params)\n    response_text = r.text\n    data = json.loads(response_text)\n\n    try:\n       # create a new dictionary for that country\n        dict = {\"COUNTRY_DESC_PS\": country,\n                \"longitude\": data[0][\"lon\"],\n                \"latitude\": data[0][\"lat\"]}\n        # add it to list of dictionaries\n        country_list_dicts.append(dict)\n\n    except:\n        pass\n    \n# turn the list of dictionaries into a pandas dataframe\ncountries = pd.DataFrame(country_list_dicts)\n\n# merge with dataframe with all student information\ndf_geo = pd.merge(df, countries, on=\"COUNTRY_DESC_PS\")\n\n\nCreate the map\n# generate a map that shows the whole world.\nm = folium.Map(location=[20, 0], zoom_start=1.5, tiles=\"OpenStreetMap\")\n# add our data to the map as points\nfor i in range(0,len(countries)):\n        # set variable to check that country matches\n        match_country = (df[\"COUNTRY_DESC_PS\"] == countries.loc[i][\"COUNTRY_DESC_PS\"])\n        # set all your variables\n        lat = countries.loc[i]['latitude']\n        long = countries.loc[i]['longitude']\n        # create summmaries from main dataframe, counts of different categories\n        # note, pep8 says lines too long, but leaving as is\n        total = int(df[(df[\"COUNTRY_DESC_PS\"] == countries.loc[i][\"COUNTRY_DESC_PS\"])]\n                    .count()[\"COUNTRY_DESC_PS\"])\n        male = df[match_country & (df[\"GENDER\"] == \"M\")].count()[\"GENDER\"]\n        female = df[match_country & (df[\"GENDER\"] == \"F\")].count()[\"GENDER\"]\n        ugrd = df[match_country & (df[\"ACAD_PROG\"] == \"UGRD\")].count()[\"ACAD_PROG\"]\n        ms = df[match_country & (df[\"ACAD_PROG\"] == \"MASTR\")].count()[\"ACAD_PROG\"]\n        phd = df[match_country & (df[\"ACAD_PROG\"] == \"DOCT\")].count()[\"ACAD_PROG\"]\n        opt = df[match_country & (df[\"OPT\"] == \"Y\")].count()[\"OPT\"]\n        # set radius, so countries with more students get bigger circles\n        radius = 1300*(total)\n        # adjust the radius size to make the largest countries smaller\n        # otherwise they overwhelm the visualization\n        if radius > 5000000:\n            radius = 150*(total)\n        elif radius > 900000:\n            radius = 500*(total)\n        elif radius < 60000:\n            radius = 60000\n        # create hover tooltip to show country name\n        tooltip_text = countries.loc[i][\"COUNTRY_DESC_PS\"]\n        # set pop up text formatting and labels\n        popup_text = \"\"\"<h4>{}</h4><br>\n                    <b>Count</b>: {}<br><br>\n                    <b>Undergraduates</b>: {}<br>\n                    <b>Master's</b>: {}<br>\n                    <b>Doctoral</b>: {}<br><br>\n                    <b>On OPT</b>: {}<br><br>\n                    <b>Male</b>: {}<br>\n                    <b>Female</b>: {}<br>\n                    \"\"\"\n        # set pop up text variables\n        popup_text = popup_text.format(countries.iloc[i]['COUNTRY_DESC_PS'],\n                                       total,\n                                       ugrd,\n                                       ms,\n                                       phd,\n                                       opt,\n                                       male,\n                                       female)\n        # generate the circle markers, populating with variables set in loop\n        folium.Circle(location=[lat, long],\n                      radius=radius,\n                      tooltip=tooltip_text,\n                      popup=popup_text,\n                      fill=True).add_to(m)\n# preview the map here\nm\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\nSave the file\nRun the following code block and enter the file name and select the location where you would like to save your output file.\n# prompt user for file name and check that it is a valid file name\nfile_name = input(\"Enter file name: \")\ncheck = True\nwhile check:\n    # check that filename does not include special characters\n    if not re.match(\"[- _a-zA-Z0-9]*$\", file_name):\n        print(\"Please only enter letters, numbers, underscores, or hyphens.\")\n        file_name = input(\"Enter file name: \")\n    # check that user actually entered a filename\n    elif not file_name:\n        print(\"Please enter a filename.\")\n        file_name = input(\"Enter file name: \")\n    else:\n        check = False\n\n# prompt user for where to save the file\ndirectory = filedialog.askdirectory()\nwhile not directory:\n    print(\"You must choose a location to save your map.\")\n    directory = filedialog.askdirectory()\n# change directory to the selected directory\nos.chdir(directory)\n\n# save map to html in selected directory\nm.save(file_name+\".html\")\nEnter file name: final_test\n\n\nYou’re done!\nYou can take the resulting HTML file to add to our WordPress instance. You can find instructions on how to update WordPress in the Digital Media Assistant manual in OneNote."
  },
  {
    "objectID": "Projects.html#python",
    "href": "Projects.html#python",
    "title": "Projects",
    "section": "Python",
    "text": "Python\n\nShowcasing Diversity: Interactive Map Generator\nProject for EPPS 6317 - Python Programming. Created a reusable python script in Jupyter Notebook to take data from OSPA report and generate an interactive map showing the country of origin for the UT Dallas international student population. The map can be saved as an HTML file, to be used on the ISSO website."
  },
  {
    "objectID": "Projects.html#sql-and-postgresql",
    "href": "Projects.html#sql-and-postgresql",
    "title": "Projects",
    "section": "SQL and PostgreSQL",
    "text": "SQL and PostgreSQL\n\nTransgender Healthcare Directory Prototype\nProject for EPPS 6354 - Information Management. For this project, I built a database using PostgreSQL, compiled sample data, and built and deployed a Shiny app."
  },
  {
    "objectID": "Projects.html#research-design",
    "href": "Projects.html#research-design",
    "title": "Projects",
    "section": "Research Design",
    "text": "Research Design\n\nResearch Design: Reducing Anti-Trans Bias Among Healthcare Providers\nProject for PPPE 6310 - Research Design. For this course, we were asked to come up with a viable research design to answer a research question. I did a literature review and developed an appropriate research design to test whether doctors’ bias towards transgender patients could be mitigated through an extended training program."
  },
  {
    "objectID": "posts/Lab03.html",
    "href": "posts/Lab03.html",
    "title": "EPPS 6323: Lab03",
    "section": "",
    "text": "R Programming (EDA)\n(Adapted from Stackoverflow examples) (Objectives: Use plotly, reshape packages, interactive visualization)\n\nlibrary(tidyverse)\nlibrary(plotly)\ndata(iris)\nattach(iris)\n# Generate plot on three quantitative variables\niris_plot <- plot_ly(iris,\n                     x = Sepal.Length,\n                     y = Sepal.Width,\n                     z = Petal.Length,\n                     type = \"scatter3d\",\n                     mode = \"markers\",\n                     size = 0.02)\niris_plot\n\n\n\n\n# Regression object\n\npetal_lm <- lm(Petal.Length ~ 0 + Sepal.Length + Sepal.Width,\n               data = iris)\nlibrary(reshape2)\n\n#load data\n\npetal_lm <- lm(Petal.Length ~ 0 + Sepal.Length + Sepal.Width,data = iris)\n\n# Setting resolution parameter\ngraph_reso <- 0.05\n\n#Setup Axis\naxis_x <- seq(min(iris$Sepal.Length), max(iris$Sepal.Length), by = graph_reso)\naxis_y <- seq(min(iris$Sepal.Width), max(iris$Sepal.Width), by = graph_reso)\n\n# Regression surface\n# Rearranging data for plotting\npetal_lm_surface <- expand.grid(Sepal.Length = axis_x,Sepal.Width = axis_y,KEEP.OUT.ATTRS = F)\npetal_lm_surface$Petal.Length <- predict.lm(petal_lm, newdata = petal_lm_surface)\npetal_lm_surface <- acast(petal_lm_surface, Sepal.Width ~ Sepal.Length, value.var = \"Petal.Length\")\nhcolors=c(\"orange\",\"blue\",\"green\")[iris$Species]\niris_plot <- plot_ly(iris,\n                     x = ~Sepal.Length,\n                     y = ~Sepal.Width,\n                     z = ~Petal.Length,\n                     text = Species,\n                     type = \"scatter3d\",\n                     mode = \"markers\",\n                     marker = list(color = hcolors),\n                     size=0.02)\n# Add surface\niris_plot <- add_trace(p = iris_plot,\n                       z = petal_lm_surface,\n                       x = axis_x,\n                       y = axis_y,\n                       type = \"surface\",mode = \"markers\",\n                       marker = list(color = hcolors))\niris_plot\n\n\n\n\n\n\n\nRegression object\n\npetal_lm <- lm(Petal.Length ~ 0 + Sepal.Length + Sepal.Width, \n               data = iris)"
  },
  {
    "objectID": "posts/Assignment3.html",
    "href": "posts/Assignment3.html",
    "title": "Assignment 3 - Regression Analysis with TEDS 2016 data",
    "section": "",
    "text": "Loading required package: pacman"
  },
  {
    "objectID": "posts/Assignment3.html#regression-with-teds-2016-data",
    "href": "posts/Assignment3.html#regression-with-teds-2016-data",
    "title": "Assignment 3 - Regression Analysis with TEDS 2016 data",
    "section": "Regression with TEDS 2016 data",
    "text": "Regression with TEDS 2016 data\n\n# read in the data\nteds <- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\nConverting atomic to factors. Please wait..."
  },
  {
    "objectID": "posts/Assignment3.html#select-only-relevant-variables",
    "href": "posts/Assignment3.html#select-only-relevant-variables",
    "title": "Assignment 3 - Regression Analysis with TEDS 2016 data",
    "section": "Select only relevant variables",
    "text": "Select only relevant variables\nCreate a subset of the dataset (Tondu, female, DPP, age, income, edu, Taiwanese and Econ_worse).\n\n# add labels to Tondu variable\nteds$Tondu<-as.numeric(teds$Tondu,labels=c(\"Unification now”, “Status quo, unif. in future”, “Status quo, decide later\", \"Status quo forever\", \"Status quo, indep. in future\", \"Independence now”, “No response\"))\n\nteds <- teds %>% \n    select(Tondu, female, DPP, age, income, edu, Taiwanese, Econ_worse)"
  },
  {
    "objectID": "posts/Assignment3.html#run-a-regplot-on-the-dependent-variable",
    "href": "posts/Assignment3.html#run-a-regplot-on-the-dependent-variable",
    "title": "Assignment 3 - Regression Analysis with TEDS 2016 data",
    "section": "5. Run a regplot on the dependent variable",
    "text": "5. Run a regplot on the dependent variable\nUse age, education, income\n\nlm(Tondu ~ 0 + age, edu, income, data = teds)\n\n\nCall:\nlm(formula = Tondu ~ 0 + age, data = teds, subset = edu, weights = income)\n\nCoefficients:\n    age  \n0.07137  \n\n\nWhat is the problem? Why? (hint: how many categories in the DV?)\nWhat can be done to improve prediction of the dependent variable?"
  },
  {
    "objectID": "posts/Assignment-2_TEDS-2016.html",
    "href": "posts/Assignment-2_TEDS-2016.html",
    "title": "Assignment-2: Exploratory Analysis of TEDS-2016 data",
    "section": "",
    "text": "Run an exploratory data analysis with R using the TEDS2016 dataset.\n\n\nLoading required package: pacman\n\n\n\n\n\n# load the data\nteds <- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\nConverting atomic to factors. Please wait...\n\n\n\n\n\n\nhead(teds)\n\n  District Sex Age Edu Arear Career Career8 Ethnic Party PartyID Tondu Tondu3\n1      201   2   4   4     1      1       1      1    25       9     3      2\n2      201   2   2   5     1      2       3      2    25       9     5      3\n3      201   1   5   5     1      1       1      2     3       1     3      2\n4      201   1   4   2     1      4       4      1    25       9     5      3\n5      201   2   5   1     1      3       5      9    25       9     9      9\n6      201   2   5   2     1      2       7      1     6       2     4      2\n  nI2 votetsai green votetsai_nm votetsai_all Independence Unification sq\n1   3       NA     0          NA            0            0           0  1\n2  98        1     0           1            1            1           0  0\n3  98        0     0           0            0            0           0  1\n4   3       NA     0          NA            0            1           0  0\n5  98       NA     0          NA            0            0           0  0\n6  98        1     1           1            1            0           0  1\n  Taiwanese edu female whitecollar lowincome income income_nm age KMT DPP npp\n1         1   4      1           1         4    8.0         8  59   0   0   0\n2         0   5      1           1         4    7.0         7  39   0   0   0\n3         0   5      0           1         5    8.0         8  63   1   0   0\n4         1   2      0           0         4    5.0         5  55   0   0   0\n5         0   1      1           0         3    5.5        NA  76   0   0   0\n6         1   2      1           1         5    9.0         9  64   0   1   0\n  noparty pfp South north Minnan_father Mainland_father Econ_worse Inequality\n1       1   0     0     1             1               0          0          1\n2       1   0     0     1             1               0          0          1\n3       0   0     0     1             1               0          1          1\n4       1   0     0     1             1               0          1          1\n5       1   0     0     1             1               0          0          0\n6       0   0     0     1             1               0          1          1\n  inequality5 econworse5 Govt_for_public pubwelf5 Govt_dont_care highincome\n1           4          3               1        5              0          1\n2           5          3               1        5              0          1\n3           5          4               1        4              1          1\n4           5          5               0        1              1          1\n5           3          3               0        3              0         NA\n6           5          4               0        2              1          1\n  votekmt votekmt_nm Blue Green No_Party voteblue voteblue_nm votedpp_1\n1       0         NA    0     0        0        0          NA        NA\n2       0          0    0     0        0        0           0         1\n3       1          1    0     0        0        1           1         0\n4       0         NA    0     0        0        0          NA        NA\n5       0         NA    0     0        0        0          NA        NA\n6       0          0    0     0        0        0           0         1\n  votekmt_1\n1        NA\n2         0\n3         1\n4        NA\n5        NA\n6         0\n\ncolnames(teds)\n\n [1] \"District\"        \"Sex\"             \"Age\"             \"Edu\"            \n [5] \"Arear\"           \"Career\"          \"Career8\"         \"Ethnic\"         \n [9] \"Party\"           \"PartyID\"         \"Tondu\"           \"Tondu3\"         \n[13] \"nI2\"             \"votetsai\"        \"green\"           \"votetsai_nm\"    \n[17] \"votetsai_all\"    \"Independence\"    \"Unification\"     \"sq\"             \n[21] \"Taiwanese\"       \"edu\"             \"female\"          \"whitecollar\"    \n[25] \"lowincome\"       \"income\"          \"income_nm\"       \"age\"            \n[29] \"KMT\"             \"DPP\"             \"npp\"             \"noparty\"        \n[33] \"pfp\"             \"South\"           \"north\"           \"Minnan_father\"  \n[37] \"Mainland_father\" \"Econ_worse\"      \"Inequality\"      \"inequality5\"    \n[41] \"econworse5\"      \"Govt_for_public\" \"pubwelf5\"        \"Govt_dont_care\" \n[45] \"highincome\"      \"votekmt\"         \"votekmt_nm\"      \"Blue\"           \n[49] \"Green\"           \"No_Party\"        \"voteblue\"        \"voteblue_nm\"    \n[53] \"votedpp_1\"       \"votekmt_1\"      \n\n\n\n\n\n\n# Prepare the analyze the Party ID variable \n# Assign label to the values (1=KMT, 2=DPP, 3=NP, 4=PFP, 5=TSU, 6=NPP, 7=\"NA\")\n\nteds$PartyID <- factor(teds$PartyID, labels=c(\"KMT\",\"DPP\",\"NP\",\"PFP\", \"TSU\", \"NPP\",\"NA\"))\n\n# Check the variable\nattach(teds)\nhead(PartyID)\n\n[1] NA  NA  KMT NA  NA  DPP\nLevels: KMT DPP NP PFP TSU NPP NA\n\ntail(PartyID)\n\n[1] NA  NA  DPP NA  NA  NA \nLevels: KMT DPP NP PFP TSU NPP NA\n\n# Let's try doing the same with the Tondu variable\nteds$Tondu <- factor(teds$Tondu, labels = get_labels(teds$Tondu))\n\n\n\n\n\n# Run a frequency table of the Party ID variable using the descr package\nfreq(teds$PartyID)\n\n\n\n\nteds$PartyID \n      Frequency  Percent\nKMT         388  22.9586\nDPP         591  34.9704\nNP            3   0.1775\nPFP          32   1.8935\nTSU           5   0.2959\nNPP          43   2.5444\nNA          628  37.1598\nTotal      1690 100.0000\n\n# Plot the Party ID variable\nteds %>% \n  count(PartyID) %>% \n  mutate(perc = n / nrow(teds)) -> T2\nggplot(T2, aes(x = reorder(PartyID, -perc),y = perc,fill=PartyID)) +\n  geom_bar(stat = \"identity\") +\n  ylab(\"Party Support (%)\") + \n  xlab(\"Taiwan Political Parties\") +\n  theme_bw() +\n  scale_fill_manual(values=c(\"steel blue\",\"forestgreen\",\"khaki1\",\"orange\",\"goldenrod\",\"yellow\",\"grey\"))\n\n\n\n\n\n\n\nWhat problems do you encounter when working with the dataset?\nI had trouble finding the codebook. Error messages about conflicting labels. The data coming from stata has labels attached to the variables. However, I had some difficulty figuring out how to view the labels directly in R, rather than referring to a codebook or manually setting the labels. I found the sjlabeller has a function called “get_labels()” that makes this very easy for variables that have this meta information included, like the Tondu variable.\n\nget_labels(teds$Tondu)\n\n[1] \"Immediate unification\"                                             \n[2] \"Maintain the status quo,move toward unification\"                   \n[3] \"Maintain the status quo, decide either unification or independence\"\n[4] \"Maintain the status quo forever\"                                   \n[5] \"Maintain the status quo,move toward independence\"                  \n[6] \"Immediate independence\"                                            \n[7] \"Nonresponse\"                                                       \n\n\nHow to deal with missing values? In general, you can either exclude or impute missing values.\n\n\nincluding female, DPP, age, income, edu, Taiwanese and Econ_worse. What methods would you use?\nTo begin exploring the data, I like to generate some simple visualizations to see the spread of the data. These are quick, unpolished visualizations just to get an idea of what we’re working with.\nIf we want to do the same visualization for multiple variables, we can wrap it in a for loop to print out our charts quickly.\n\n# Binary variables bar charts\nvar_list <- c(\"female\", \"DPP\", \"Taiwanese\", \"Econ_worse\")\n\nfor (var in var_list) {\n    chart <- teds %>%\n        ggplot(aes(x=Tondu, y=!!sym(var)), fill=!!sym(var))+\n        geom_bar(stat=\"identity\") +\n        coord_flip() + \n        labs(title=paste(\"Tondu and \",var))\n    \n    print(chart)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n# Tondu and age\n# let's find the mean age, grouped by Tondu\n\nteds %>% \n    group_by(Tondu) %>% \n    summarize(m = mean(age)) %>%\n    ggplot(aes(x=Tondu, y=m)) + \n    geom_bar(stat = \"identity\") +\n    coord_flip()\n\n\n\n# Tondu and income\nteds %>% \n    group_by(Tondu) %>% \n    summarize(m = mean(income)) %>%\n    ggplot(aes(x=Tondu, y=m)) + \n    geom_bar(stat = \"identity\") +\n    coord_flip()\n\n\n\n# Tondu and edu\n# multiple levels of variable, no baked in labels\nteds %>% \n    ggplot(aes(x=Tondu)) +\n    geom_bar() +\n    coord_flip() +\n    facet_wrap(~Edu)\n\n\n\n# Tondu and Taiwanese\n# binary variable\nteds %>% \n    ggplot(aes(x=Tondu)) +\n    geom_bar() +\n    coord_flip() +\n    facet_wrap(~Taiwanese)\n\n\n\n# Tondu and econ-worse\n# binary variable\nteds %>% \n    ggplot(aes(x=Tondu)) +\n    geom_bar() +\n    coord_flip() +\n    facet_wrap(~Econ_worse)\n\n\n\n# Tondu and votesai variable\nteds %>% \n    ggplot(aes(x=Tondu)) +\n    geom_bar() +\n    coord_flip() +\n    facet_wrap(~votetsai)\n\n\n\n\n\n\n\nAssign labels to the variable using the following:\n\nteds$Tondu<-as.numeric(teds$Tondu,labels=c(\"Unification now”, “Status quo, unif. in future”, “Status quo, decide later\", \"Status quo forever\", \"Status quo, indep. in future\", \"Independence now”, “No response\"))\n\nCreate the table and bar chart\n\n# frequency table\n\nfreq(teds$Tondu)\n\n\n\n\nteds$Tondu \n      Frequency Percent\n1            27   1.598\n2           180  10.651\n3           546  32.308\n4           328  19.408\n5           380  22.485\n6           108   6.391\n7           121   7.160\nTotal      1690 100.000\n\n# Plot the Party ID variable\nteds %>% \n  count(Tondu) %>% \n  mutate(perc = n / nrow(teds)) -> T2\nggplot(T2, aes(x = reorder(Tondu, -perc),y = perc,fill=Tondu)) +\n  geom_bar(stat = \"identity\") +\n  ylab(\"Tondu (percentage)\") + \n  xlab(\"\") +\n  theme_bw()"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Project for EPPS 6356 - Data Visualization. For this project, I used the TransPop dataset (downloaded from ICPSR). I used the survey and srvyr packages to work with weighted data and developed visualizations using ggplot2.\n\n\n\nProject for EPPS 6316 - Regression Analysis. In this course, we were tasked with finding a study and replicating its results using R, then extending the study with our own regression analysis. I replicated Burford et al’s study Associations of Urbanicity and Sociodemographic Characteristics with Protective Health Behaviors and Reasons for Leaving the Home During COVID-19. You can download my dataset and R script."
  },
  {
    "objectID": "projects/index.html#python",
    "href": "projects/index.html#python",
    "title": "Projects",
    "section": "Python",
    "text": "Python\n\nShowcasing Diversity: Interactive Map Generator\nProject for EPPS 6317 - Python Programming. Created a reusable python script in Jupyter Notebook to take data from OSPA report and generate an interactive map showing the country of origin for the UT Dallas international student population. The map can be saved as an HTML file, to be used on the ISSO website."
  },
  {
    "objectID": "projects/index.html#sql-and-postgresql",
    "href": "projects/index.html#sql-and-postgresql",
    "title": "Projects",
    "section": "SQL and PostgreSQL",
    "text": "SQL and PostgreSQL\n\nTransgender Healthcare Directory Prototype\nProject for EPPS 6354 - Information Management. For this project, I built a database using PostgreSQL, compiled sample data, and built and deployed a Shiny app."
  },
  {
    "objectID": "projects/index.html#research-design",
    "href": "projects/index.html#research-design",
    "title": "Projects",
    "section": "Research Design",
    "text": "Research Design\n\nResearch Design: Reducing Anti-Trans Bias Among Healthcare Providers\nProject for PPPE 6310 - Research Design. For this course, we were asked to come up with a viable research design to answer a research question. I did a literature review and developed an appropriate research design to test whether doctors’ bias towards transgender patients could be mitigated through an extended training program."
  }
]